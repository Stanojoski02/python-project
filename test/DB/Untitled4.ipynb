{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82653342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOjan\n",
      "grouping_sentences function is running\n",
      "Sentence: 500\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 1000\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 1500\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 2000\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 2500\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 3000\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 3500\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 4000\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 4500\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 5000\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 5500\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 6000\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 6500\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n",
      "Sentence: 7000\n",
      "write_sentences_in_sqlite function is running\n",
      "0 email addresses are recorded\n",
      "100 email addresses are recorded\n",
      "200 email addresses are recorded\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "import pandas\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "import pylev\n",
    "from math import sqrt, pow\n",
    "import numpy\n",
    "import datetime\n",
    "import openpyxl\n",
    "import sys\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "sys.setrecursionlimit(170000)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def squared_sum(x):\n",
    "    \"\"\" return 3 rounded square rooted value \"\"\"\n",
    "    return round(sqrt(sum([a * a for a in x])), 3)\n",
    "\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    \"\"\" return euclidean distance between two lists \"\"\"\n",
    "    return sqrt(sum(pow(a - b, 2) for a, b in zip(x, y)))\n",
    "\n",
    "\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def jaccard_similarity(x, y):\n",
    "    \"\"\" returns the jaccard similarity between two lists \"\"\"\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality / float(union_cardinality)\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    exclude.remove('@')\n",
    "\n",
    "    # remove new line and digits with regular expression\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "\n",
    "    # remove patterns matching url format\n",
    "    url_pattern = r'((http|ftp|https):\\/\\/)?[\\w\\-]+(\\.[\\w\\-]+)+([\\w\\-\\.,@?^=%&amp;:/\\+#]*[\\w\\-\\@?^=%&amp;/\\+#])?'\n",
    "    text = re.sub(url_pattern, ' ', text)\n",
    "\n",
    "    # remove non-ascii characters, all below the ordinal coding of 8000 are allowed, so times all emojis gone\n",
    "    text = ''.join(character for character in text if ord(character) < 8000)\n",
    "\n",
    "    # remove punctuations # unfortunately the @ sign goes with it, so explicitly remove the @ sign from the\n",
    "    # exclude list\n",
    "    text = ''.join(character for character in text if character not in exclude)\n",
    "\n",
    "    # standardize white space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # drop capitalization\n",
    "    text = text.lower()\n",
    "    # now remove all twitter usernames, they have the structure @username,\n",
    "    # so explicitly leave the @ above to identify the usernames\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "\n",
    "    # remove white space, this is the last step, because after removing the\n",
    "    # usernames there are often empty characters left\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def sentence_extractor(string_):\n",
    "    sentences_list = []\n",
    "    data = sent_tokenize(string_)\n",
    "    for sentence in data:\n",
    "        txt = \"\"\n",
    "        sentence = re.sub('<[^>]+>', '', sentence).strip()\n",
    "        if \"Von:\" not in sentence and \\\n",
    "                \"An:\" not in sentence and \\\n",
    "                \"From:\" not in sentence and \\\n",
    "                \"Web:\" not in sentence and \\\n",
    "                len(sentence.split()) < 30:\n",
    "            for word in sentence.split():\n",
    "                if word[0].isalpha() and \\\n",
    "                        (\"@\" not in word) and \\\n",
    "                        (\".com\" not in word) and \\\n",
    "                        (\"http\" not in word) and \\\n",
    "                        (\"www\" not in word) and \\\n",
    "                        len(word) < 24:\n",
    "                    pass\n",
    "                else:\n",
    "                    word = \"\"\n",
    "                txt += \" {}\".format(word).replace(\":\", \"\")\n",
    "        if txt and len(txt.split()) > 2 and (txt[-1] == \"!\" or txt[-1] == \".\" or txt[-1] == \"?\"):\n",
    "            txt = cleaning(txt)\n",
    "            sentences_list.append(txt.strip())\n",
    "    return sentences_list\n",
    "\n",
    "\n",
    "def sentence_formatting(_input, _output):\n",
    "    email_num = 0\n",
    "    data = pd.read_csv(_input, encoding='charmap')\n",
    "    data_lines = data.values.tolist()\n",
    "    line_num = 0\n",
    "    print(\"sentence_formatting function is running\")\n",
    "    for line in data_lines:\n",
    "        try:\n",
    "            for i in sentence_extractor(line[4]):\n",
    "                try:\n",
    "                    if line_num % 500 == 0:\n",
    "                        print(f\"Formatted sentences {line_num}\")\n",
    "                    line_num += 1\n",
    "                    doc = nlp(str(i))\n",
    "                    propn = 0\n",
    "                    #               propn: proper noun\n",
    "                    verb = 0\n",
    "                    #               verb:  Word used to describe an action\n",
    "                    adj = 0\n",
    "                    #               adjective: A word naming an attribute of a noun, such as sweet, red\n",
    "                    noun = 0\n",
    "                    #               noun: A word used to identify any of a class of people, places, or things\n",
    "                    for token in doc:\n",
    "                        if token.pos_ == \"PROPN\":\n",
    "                            propn += 1\n",
    "                        elif token.pos_ == \"VERB\":\n",
    "                            verb += 1\n",
    "                        elif token.pos_ == \"ADJ\":\n",
    "                            adj += 1\n",
    "                        elif token.pos_ == \"NOUN\":\n",
    "                            noun += 1\n",
    "                    finished_line = f\"{email_num},\" \\\n",
    "                                    f\"DateTime: {line[0]},\" \\\n",
    "                                    f\" From: {line[1]},\" \\\n",
    "                                    f\" To: {line[2]},\" \\\n",
    "                                    f\" Sentence:  {i.strip()},\" \\\n",
    "                                    f\" PROPN: {propn},\" \\\n",
    "                                    f\" VERB: {verb},\" \\\n",
    "                                    f\" ADJ: {adj},\" \\\n",
    "                                    f\" NOUN: {noun},\" \\\n",
    "                                    f\"{line[3]}\\n\"\n",
    "                    with open(_output, \"a\", encoding=\"charmap\") as a:\n",
    "                        a.write(finished_line)\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "        email_num += 1\n",
    "\n",
    "\n",
    "def grouping_sentences(_input, _output):\n",
    "    sen = []\n",
    "    print(\"grouping_sentences function is running\")\n",
    "    line_num = 0\n",
    "    with open(_input, \"r\", encoding=\"charmap\") as d:\n",
    "        if str(type(_input)) == \"<class 'list'>\":\n",
    "            data_lines = _input\n",
    "        else:\n",
    "            data_lines = d.readlines()\n",
    "        sentences = []\n",
    "        numm = 0\n",
    "        num_of_sentence = 500\n",
    "        for line in data_lines:\n",
    "            numm += len(sentences)\n",
    "            if num_of_sentence <= numm:\n",
    "                print(numm)\n",
    "                num_of_sentence += 500\n",
    "            sentences = []\n",
    "            if line_num % 500 == 0:\n",
    "                print(f\"Grouping is up to {line_num} lines\")\n",
    "            line_num += 1\n",
    "            for line_2 in data_lines:\n",
    "\n",
    "                if jaccard_similarity(line.split(\",\")[4].split(), line_2.split(\",\")[4].split()) > 0.2:\n",
    "                    if line_2.split(',')[4] not in sen:\n",
    "                        try:\n",
    "                            sentences.append(f\"{line_2.split(',')[0]},\"\n",
    "                                             f\"{jaccard_similarity(line.split(',')[4].split(), line_2.split(',')[4].split())},\"\n",
    "                                             f\"{get_cosine(text_to_vector(line.split(',')[4]), text_to_vector(line_2.split(',')[4]))},\"\n",
    "                                             f\"{pylev.levenshtein(line.split(',')[4].split(), line_2.split(',')[4].split())},\"\n",
    "                                             f\"{euclidean_distance(nlp(line.split(',')[4]).vector, nlp(line_2.split(',')[4]).vector)},\"\n",
    "                                             f\"{line_2}\\n\")\n",
    "                            sen.append(line_2.split(',')[4])\n",
    "                        except:\n",
    "                            pass\n",
    "            if len(sentences) > 1:\n",
    "\n",
    "                for sentence in sentences:\n",
    "                    with open(_output, \"a\", encoding=\"charmap\") as writer:\n",
    "                        writer.write(sentence)\n",
    "\n",
    "\n",
    "def test_function(_input, _output, number_of_sentences):\n",
    "    sen = []\n",
    "    begin_time = datetime.datetime.now()\n",
    "    print(\"grouping_sentences function is running\")\n",
    "    line_num = 0\n",
    "    with open(_input, \"r\", encoding=\"charmap\") as d:\n",
    "        data_lines = d.readlines()\n",
    "        num_of_sentences = 0\n",
    "        for line in data_lines:\n",
    "            sentences = []\n",
    "            num_of_sentences_in_group = 0\n",
    "            if num_of_sentences >= number_of_sentences:\n",
    "                time_for_running = datetime.datetime.now() - begin_time\n",
    "                print(\n",
    "                    f\"{int(number_of_sentences / time_for_running.total_seconds() * 60)} sentences per minute are grouped\")\n",
    "                print(f\"{int(number_of_sentences / time_for_running.total_seconds())} sentences per second are grouped\")\n",
    "                break\n",
    "            line_num += 1\n",
    "            for line_2 in data_lines:\n",
    "                if jaccard_similarity(line.split(\",\")[4].split(), line_2.split(\",\")[4].split()) > 0.2:\n",
    "                    if line_2.split(',')[4] not in sen:\n",
    "                        try:\n",
    "                            sentences.append(f\"{line.split(',')[0]},\"\n",
    "                                             f\"{jaccard_similarity(line.split(',')[4].split(), line_2.split(',')[4].split())},\"\n",
    "                                             f\"{get_cosine(text_to_vector(line.split(',')[4]), text_to_vector(line_2.split(',')[4]))},\"\n",
    "                                             f\"{pylev.levenshtein(line.split(',')[4].split(), line_2.split(',')[4].split())},\"\n",
    "                                             f\"{euclidean_distance(nlp(line.split(',')[4]).vector, nlp(line_2.split(',')[4]).vector)},\"\n",
    "                                             f\"{line_2}\\n\")\n",
    "                            sen.append(line_2.split(',')[4])\n",
    "                            num_of_sentences_in_group += 1\n",
    "\n",
    "                        except:\n",
    "                            pass\n",
    "            if len(sentences) > 1:\n",
    "                num_of_sentences += num_of_sentences_in_group\n",
    "                for sentence in sentences:\n",
    "                    with open(_output, \"a\", encoding=\"charmap\") as writer:\n",
    "                        writer.write(sentence)\n",
    "\n",
    "\n",
    "def write_sentences_in_sqlite(input_, output_, file_num):\n",
    "    print(\"write_sentences_in_sqlite function is running\")\n",
    "\n",
    "    with open(input_, \"r\", encoding=\"charmap\") as d:\n",
    "        if str(type(input_)) == \"<class 'list'>\":\n",
    "            sentences = input_\n",
    "        else:\n",
    "            sentences = d.readlines()\n",
    "\n",
    "        emails = []\n",
    "        data = []\n",
    "        email_1 = \"\"\n",
    "        for sentence in sentences:\n",
    "            if sentence.split(\",\") != ['\\n']:\n",
    "                try:\n",
    "                    if sentence == sentences[0]:\n",
    "                        continue\n",
    "                    email_1 = sentence.strip().split(\",\")[7].replace('\"', \"\").replace(\"From:\", \"\").replace('\"', '').strip().lower()\n",
    "\n",
    "                    if \";\" in email_1:\n",
    "                        email_1 = email_1.split(';')[0].strip()\n",
    "                    email_2 = sentence.split(\",\")[8].replace('\"', \"\").replace(\"To:\", \"\").replace('\"', '').strip().lower()\n",
    "                    if \";\" in email_2:\n",
    "                        email_2 = email_2.split(';')[0].strip()\n",
    "                    if email_1 not in emails:\n",
    "                        emails.append(email_1)\n",
    "                    if email_2 not in emails:\n",
    "                        emails.append(email_2)\n",
    "\n",
    "                    data.append([\n",
    "                        sentence.split(\",\")[0].replace('\"', \"\"),\n",
    "                        sentence.split(\",\")[6].replace('\"', \"\").replace(\"DateTime:\", \"\"),\n",
    "                        \"EMID\"+ str(sentence.split(\",\")[0].replace('\"', \"\")),\n",
    "                        emails.index(email_1),\n",
    "                        emails.index(email_2),\n",
    "                        sentence.split(\",\")[9].replace(\"Sentence:\", \"\").strip(),\n",
    "                        sentence.split(\",\")[10].replace(\"PROPN:\", \"\"),\n",
    "                        sentence.split(\",\")[11].replace(\"VERB:\", \"\"),\n",
    "                        sentence.split(\",\")[12].replace(\"ADJ:\", \"\"),\n",
    "                        sentence.split(\",\")[13].replace(\"NOUN:\", \"\"),\n",
    "                        numpy.round(float(sentence.split(\",\")[1]), 4),\n",
    "                        numpy.round(float(sentence.split(\",\")[2]), 4),\n",
    "                        sentence.split(\",\")[3],\n",
    "                        numpy.round(float(sentence.split(\",\")[4]), 4)\n",
    "                    ])\n",
    "                except:\n",
    "                    raise Exception\n",
    "\n",
    "        df = pd.DataFrame(data, columns=[\n",
    "            \"ID\", \"date\", \"email_id\", \"from(email index)\",\n",
    "            \"to(email index)\", \"sentence\", \"propn\", \"verb\",\n",
    "            \"adjective\", \"noun\", \"jaccard_similarity\",\n",
    "            \"cosine_similarity\", \"levenshtein_distance\",\n",
    "            \"euclidean_distance\"\n",
    "        ])\n",
    "\n",
    "        conn = sqlite3.connect(output_)\n",
    "        df.to_sql(f\"tbl_sentence_{file_num}\", conn, if_exists=\"replace\", index=False)\n",
    "        conn.close()\n",
    "        ID = 1\n",
    "        number_of_emails = 0\n",
    "\n",
    "        # Connect to the SQLite database\n",
    "        conn = sqlite3.connect(output_)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create the table if it doesn't exist\n",
    "        cursor.execute(f'''\n",
    "            CREATE TABLE IF NOT EXISTS emails_{file_num} (\n",
    "                ID INTEGER,\n",
    "                email TEXT\n",
    "            )\n",
    "        ''')\n",
    "\n",
    "        # Insert the emails into the table\n",
    "        for email in emails:\n",
    "            try:\n",
    "                if email != emails[0]:\n",
    "                    if number_of_emails % 100 == 0:\n",
    "                        print(f\"{number_of_emails} email addresses are recorded\")\n",
    "                    number_of_emails += 1\n",
    "                    cursor.execute(f\"INSERT INTO emails_{file_num} (ID, email) VALUES (?, ?)\", (ID, email))\n",
    "                    ID += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Commit the changes and close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        sentence_list = []\n",
    "        email_ids = []\n",
    "        ID = 0\n",
    "\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(output_)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create the email table if it doesn't exist\n",
    "        cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS tbl_email_{file_num} (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            email_id TEXT NOT NULL,\n",
    "            email_date TEXT NOT NULL,\n",
    "            email_time TEXT NOT NULL,\n",
    "            email_subject TEXT,\n",
    "            from_id INTEGER NOT NULL,\n",
    "            to_id INTEGER NOT NULL,\n",
    "            body TEXT NOT NULL\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Iterate over the sentences\n",
    "        for sentence in sentences:\n",
    "            sentence_list = []\n",
    "            email_id = sentence.split(\",\")[0].replace('\"', \"\")\n",
    "            if email_id not in email_ids:\n",
    "                email_ids.append(email_id)\n",
    "            try:    \n",
    "                for s in sentences:\n",
    "                    s_email_id = s.split(\",\")[0].replace('\"', \"\")\n",
    "                    if s_email_id == email_id and s.split(\",\")[9].replace(\"Sentence:\", \"\").strip() not in sentence_list:\n",
    "                        sentence_list.append(s.split(\",\")[9].replace(\"Sentence:\", \"\").strip())\n",
    "                email_1 = sentence.split(\",\")[7].replace('\"', \"\").replace(\"From:\", \"\").replace('\"', '').strip().lower()\n",
    "                email_1 = email_1.split(';')[0].strip() if \";\" in email_1 else email_1\n",
    "                email_2 = sentence.split(\",\")[8].replace('\"', \"\").replace(\"To:\", \"\").replace('\"', '').strip().lower()\n",
    "                email_2 = email_2.split(';')[0].strip() if \";\" in email_2 else email_2\n",
    "                subject = sentence.split(\",\")[14] if len(sentence.split(\",\")) > 14 else ''\n",
    "                body = str(sentence_list).replace(\"]\", \"\").replace(\"[\", \"\")\n",
    "                email_date = sentence.split(\",\")[6].replace('\"', \"\").replace(\"DateTime:\", \"\").split()[0]\n",
    "                email_time = sentence.split(\",\")[6].replace('\"', \"\").replace(\"DateTime:\", \"\").split()[1]\n",
    "                # Insert the email data into the database\n",
    "                try:\n",
    "                    cursor.execute(f\"\"\"\n",
    "                    INSERT INTO tbl_email_{file_num} (email_id, email_date, email_time, email_subject, from_id, to_id, body)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                    \"\"\", (\"EMID\" + email_id, email_date, email_time, subject, emails.index(email_1), emails.index(email_2), body))\n",
    "                    ID += 1\n",
    "                except:\n",
    "                    pass\n",
    "            except:\n",
    "                pass\n",
    "        # Commit the changes to the database and close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def communication_streams(output_, file_num):\n",
    "    conn = sqlite3.connect(output_)\n",
    "    data = pd.read_sql(f\"SELECT * FROM tbl_email_{file_num}\", conn)\n",
    "    list_of_emails = data.values.tolist()\n",
    "    sorted_list = []\n",
    "    com_s = 0\n",
    "    sorted_communication_stream = []\n",
    "    for i in list_of_emails:\n",
    "        try:\n",
    "            for j in list_of_emails:\n",
    "                if (i[5] == j[5] or j[5] == j[6]) and (i[6] == j[5] or i[6] == j[6]) and j[\n",
    "                    1] not in sorted_communication_stream:\n",
    "                    sorted_list.append((com_s, j))\n",
    "                    sorted_communication_stream.append(j[1])\n",
    "            com_s += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    db = pd.DataFrame({\n",
    "        \"id\": [sorted_list.index(sorted_list[0])],\n",
    "        \"comm_stream_id\": [\"COMM\"+str(sorted_list[0][0])],\n",
    "        \"email_id\": [sorted_list[0][1][1]]\n",
    "    })\n",
    "\n",
    "    for comunication_stream in sorted_list:\n",
    "        if comunication_stream != sorted_list[0]:\n",
    "            try:\n",
    "                new_db = pd.DataFrame({\n",
    "                    \"id\": [sorted_list.index(comunication_stream)],\n",
    "                    \"comm_stream_id\": [\"COMM\"+str(comunication_stream[0])],\n",
    "                    \"email_id\": [comunication_stream[1][1]],\n",
    "                })\n",
    "                db = pd.concat([db, new_db], ignore_index=True, axis=0)\n",
    "            except:\n",
    "                pass\n",
    "    try:\n",
    "        db.to_sql(f\"communication_streams_{file_num}\", conn, if_exists=\"replace\", index=False)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def old_sentence_formatting(_input, _output):\n",
    "    email_num = 0\n",
    "    with open(_input, \"r\", encoding=\"charmap\") as data:\n",
    "        data_lines = data.readlines()\n",
    "        line_num = 0\n",
    "        print(\"sentence_formatting function is running\")\n",
    "        for line in data_lines:\n",
    "            try:\n",
    "                for i in sentence_extractor(line.split(\",\")[4]):\n",
    "                    try:\n",
    "                        if line_num % 500 == 0:\n",
    "                            print(f\"Sorted and formatted sentences {line_num}\")\n",
    "                        line_num += 1\n",
    "                        doc = nlp(str(i))\n",
    "                        propn = 0\n",
    "                        #               propn: proper noun\n",
    "                        verb = 0\n",
    "                        #               verb:  Word used to describe an action\n",
    "                        adj = 0\n",
    "                        #               adjective: A word naming an attribute of a noun, such as sweet, red\n",
    "                        noun = 0\n",
    "                        #               noun: A word used to identify any of a class of people, places, or things\n",
    "                        for token in doc:\n",
    "                            if token.pos_ == \"PROPN\":\n",
    "                                propn += 1\n",
    "                            elif token.pos_ == \"VERB\":\n",
    "                                verb += 1\n",
    "                            elif token.pos_ == \"ADJ\":\n",
    "                                adj += 1\n",
    "                            elif token.pos_ == \"NOUN\":\n",
    "                                noun += 1\n",
    "                        finished_line = f\"{email_num},\" \\\n",
    "                                        f\"DateTime: {line.split(',')[0]},\" \\\n",
    "                                        f\" From: {line.split(',')[1]},\" \\\n",
    "                                        f\" To: {line.split(',')[2]},\" \\\n",
    "                                        f\" Sentence:  {i.strip()},\" \\\n",
    "                                        f\" PROPN: {propn},\" \\\n",
    "                                        f\" VERB: {verb},\" \\\n",
    "                                        f\" ADJ: {adj},\" \\\n",
    "                                        f\" NOUN: {noun},\" \\\n",
    "                                        f\"{line.split(',')[3]}\\n\"\n",
    "                        with open(_output, \"a\", encoding=\"charmap\") as a:\n",
    "                            a.write(finished_line)\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            email_num += 1\n",
    "\n",
    "\n",
    "def new_func(data, sentences):\n",
    "    new_data = data\n",
    "    real_num = 0\n",
    "    if data:\n",
    "        for i in new_data:\n",
    "            group = []\n",
    "            for j in data:\n",
    "                if jaccard_similarity(i.split(',')[4].replace('Sentence:', '').strip().split(),\n",
    "                                      j.split(',')[4].replace('Sentence:', '').strip().split()) > 0.2 and j.split(',')[\n",
    "                    4].replace('Sentence:', '').strip() not \\\n",
    "                        in sentences:\n",
    "                    sentences.append(j.split(',')[4].replace('Sentence:', '').strip())\n",
    "                    group.append((f\"{j.split(',')[0]},\"\n",
    "                                  f\"{jaccard_similarity(j.split(',')[4].split(), i.split(',')[4].split())},\"\n",
    "                                  f\"{get_cosine(text_to_vector(i.split(',')[4]), text_to_vector(j.split(',')[4]))},\"\n",
    "                                  f\"{pylev.levenshtein(i.split(',')[4].split(), j.split(',')[4].split())},\"\n",
    "                                  f\"{euclidean_distance(nlp(i.split(',')[4]).vector, nlp(j.split(',')[4]).vector)},\"\n",
    "                                  f\"{j}\\n\", j))\n",
    "\n",
    "            if len(group) > 2:\n",
    "                for s in group:\n",
    "                    with open(\"sentence_in_groups.txt\", \"a\") as nsn:\n",
    "                        nsn.write(f\"{s[0]}\")\n",
    "                        new_data.remove(s[1])\n",
    "\n",
    "            else:\n",
    "                new_data.remove(i)\n",
    "            new_func(new_data, sentences)\n",
    "    else:\n",
    "        print(\"Finish!\")\n",
    "\n",
    "\n",
    "def new_grouping_sentences(_input, _output, sentence_file_name):\n",
    "    sen = []\n",
    "    bg = 500\n",
    "    file_num = 0\n",
    "    print(\"grouping_sentences function is running\")\n",
    "    line_num = 0\n",
    "    with open(_input, \"r\", encoding=\"charmap\") as d:\n",
    "        if str(type(_input)) == \"<class 'list'>\":\n",
    "            data_lines = _input\n",
    "        else:\n",
    "            data_lines = d.readlines()\n",
    "        sentences = []\n",
    "        numm = 0\n",
    "        num_of_sentence = 500\n",
    "        real_num = 0\n",
    "        for line in data_lines:\n",
    "            numm += len(sentences)\n",
    "            sentences = []\n",
    "            for line_2 in data_lines:\n",
    "\n",
    "                if jaccard_similarity(line.split(\",\")[4].split(), line_2.split(\",\")[4].split()) > 0.2:\n",
    "                    if line_2.split(',')[4] not in sen:\n",
    "                        try:\n",
    "                            sentences.append(f\"{line_2.split(',')[0]},\"\n",
    "                                             f\"{jaccard_similarity(line.split(',')[4].split(), line_2.split(',')[4].split())},\"\n",
    "                                             f\"{get_cosine(text_to_vector(line.split(',')[4]), text_to_vector(line_2.split(',')[4]))},\"\n",
    "                                             f\"{pylev.levenshtein(line.split(',')[4].split(), line_2.split(',')[4].split())},\"\n",
    "                                             f\"{euclidean_distance(nlp(line.split(',')[4]).vector, nlp(line_2.split(',')[4]).vector)},\"\n",
    "                                             f\"{line_2}\\n\")\n",
    "                            sen.append(line_2.split(',')[4])\n",
    "                        except:\n",
    "                            pass\n",
    "            if len(sentences) > 1:\n",
    "                for sentence in sentences:\n",
    "                    with open(_output, \"a\", encoding=\"charmap\") as writer:\n",
    "                        writer.write(sentence)\n",
    "                        real_num += 1\n",
    "                        if real_num % 500 == 0:\n",
    "                            print(f\"Sentence: {real_num}\")\n",
    "                            write_sentences_in_sqlite(_output,\n",
    "                                                     \"tablesAs.db\",\n",
    "                                                     file_num)\n",
    "                            communication_streams(f'tablesAs.db', file_num)\n",
    "                            bg += 500\n",
    "                            file_num += 1\n",
    "                            with open(_output, 'w') as dell:\n",
    "                                dell.write(\"\")\n",
    "\n",
    "\n",
    "def final_function(_input, _output, working_file_with_formated_sentences, working_file_sorted_sentences):\n",
    "#     try:\n",
    "#         sentence_formatting(_input, working_file_with_formated_sentences)\n",
    "#     except:\n",
    "#         old_sentence_formatting(_input, working_file_with_formated_sentences)\n",
    "    print(\"BOjan\")\n",
    "    new_grouping_sentences(working_file_with_formated_sentences, working_file_sorted_sentences, 'tbl_sentence')\n",
    "\n",
    "\n",
    "data_with_emails = \"emails.csv\"\n",
    "tbl_sentence = 'tbl_sentence.xlsx'\n",
    "formated_sentence = \"filee.txt\"\n",
    "sentence_in_groups = \"sentence_in_groupssss.txt\"\n",
    "\n",
    "final_function(data_with_emails, tbl_sentence, formated_sentence, sentence_in_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac20e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('tablesAs.db')\n",
    "\n",
    "# Read the table into a DataFrame\n",
    "df = pd.read_sql_query(\"SELECT * from tbl_sentence_0\", conn)\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()\n",
    "\n",
    "# Display the contents of the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976fda15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
